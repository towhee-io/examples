{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8347485f",
   "metadata": {},
   "source": [
    "# Build a Video Classification System in 5 Lines\n",
    "\n",
    "This notebook illustrates how to build a video classification system from scratch using [Towhee](https://towhee.io/). A video classification system classifies videos into pre-defined categories. This tutorial will use pretrained labels of human activities as example.\n",
    "\n",
    "Using the sample data of different classes of human activites, we will build a basic video classification system within 5 lines of code and check the performance using Towhee. In addition, this tutorial also suggests some optimization options. At the end, we use [Gradio](https://gradio.app/) to create a showcase that can be played with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569571ec",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "### Install packages\n",
    "\n",
    "Make sure you have installed required python packages:\n",
    "\n",
    "| package |\n",
    "| -- |\n",
    "| pymilvus |\n",
    "| towhee |\n",
    "| towhee.models |\n",
    "| pillow |\n",
    "| ipython |\n",
    "| gradio |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d8e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install -q pymilvus towhee towhee.models pillow ipython gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef6b1a",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "This tutorial will use a small data extracted from validation data of [Kinetics400](https://www.deepmind.com/open-source/kinetics). You can download the subset from [Github](https://github.com/towhee-io/data/releases/download/video-data/reverse_video_search.zip). This tutorial will just use 200 videos under `train` as example.\n",
    "\n",
    "The data is organized as follows:\n",
    "- **train:** 20 classes, 10 videos per class (200 in total)\n",
    "- **reverse_video_search.csv:** a csv file containing an ***id***, ***path***, and ***label*** for each video in train directory\n",
    "\n",
    "Let's take a quick look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54568b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -L https://github.com/towhee-io/examples/releases/download/data/reverse_video_search.zip -O\n",
    "! unzip -q -o reverse_video_search.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "490d1379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                          path                 label\n",
      "0   0  ./train/country_line_dancing/bTbC3w_NIvM.mp4  country_line_dancing\n",
      "1   1  ./train/country_line_dancing/n2dWtEmNn5c.mp4  country_line_dancing\n",
      "2   2  ./train/country_line_dancing/zta-Iv-xK7I.mp4  country_line_dancing\n",
      "country_line_dancing     10\n",
      "pumping_fist             10\n",
      "playing_trombone         10\n",
      "shuffling_cards          10\n",
      "tap_dancing              10\n",
      "clay_pottery_making      10\n",
      "eating_hotdog            10\n",
      "eating_carrots           10\n",
      "juggling_soccer_ball     10\n",
      "juggling_fire            10\n",
      "javelin_throw            10\n",
      "dunking_basketball       10\n",
      "chopping_wood            10\n",
      "trimming_trees           10\n",
      "using_segway             10\n",
      "pushing_cart             10\n",
      "dancing_gangnam_style    10\n",
      "riding_mule              10\n",
      "drop_kicking             10\n",
      "doing_aerobics           10\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./reverse_video_search.csv')\n",
    "print(df.head(3))\n",
    "print(df.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171f2e7",
   "metadata": {},
   "source": [
    "For later steps to easier get videos & measure results, we build some helpful functions in advance:\n",
    "- **ground_truth:** get ground-truth label for the video by its path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1b0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth(path):\n",
    "    label = df.set_index('path').at[path, 'label']\n",
    "    return [label.replace('_', ' ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30aa33c",
   "metadata": {},
   "source": [
    "## Build System\n",
    "\n",
    "Now we are ready to build a video classification system using sample data. We will use the [X3D_M](https://arxiv.org/abs/2004.04730) model to predict most possible action labels for input videos. With proper [Towhee operators](https://towhee.io/operators), you don't need to go through video preprocessing & model details. It is very simple to use the [method-chaining style API](https://towhee.readthedocs.io/en/main/index.html) to wrap operators and then apply them to batch inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1da9c9",
   "metadata": {},
   "source": [
    "### Predict labels\n",
    "\n",
    "Let's take some 'tap_dancing' videos as example to see how to predict labels for videos within 5 lines. By default, the system will predict top 5 labels sorting by scores (of possibility) from high to low. You can control the number of labels returnbed by change `topk`. Please note that the first time run will take some time to download model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea759389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mengjia.gu/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border-collapse: collapse;\"><tr><th style=\"text-align: center; font-size: 130%; border: none;\">path</th> <th style=\"text-align: center; font-size: 130%; border: none;\">predicts</th> <th style=\"text-align: center; font-size: 130%; border: none;\">scores</th></tr> <tr><td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">./train/tap_dancing/PehoEu4WfEI....</td> <td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[tap dancing,zumba,breakdancing,dancing gangnam style,...] len=5</td> <td style=\"text-align: left; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[0.00469,0.0029,0.0026,0.00257,...] len=5</td></tr> <tr><td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">./train/tap_dancing/X7k8twydJIU....</td> <td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[robot dancing,tap dancing,breakdancing,krumping,...] len=5</td> <td style=\"text-align: left; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[0.00542,0.00279,0.00265,0.00255,...] len=5</td></tr> <tr><td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">./train/tap_dancing/Krh21z_zyV8....</td> <td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[tap dancing,dancing ballet,roller skating,dancing charleston,...] len=5</td> <td style=\"text-align: left; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[0.00673,0.0025,0.00249,0.00249,...] len=5</td></tr> <tr><td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">./train/tap_dancing/Uf1PiOF8Poc....</td> <td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[tap dancing,dancing ballet,country line dancing,dancing charleston,...] len=5</td> <td style=\"text-align: left; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[0.0045,0.00362,0.00256,0.0025,...] len=5</td></tr> <tr><td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">./train/tap_dancing/PGPn8WhG3pM....</td> <td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[tap dancing,dancing charleston,country line dancing,jumpstyle dancing,...] len=5</td> <td style=\"text-align: left; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[0.00578,0.0029,0.0025,0.00249,...] len=5</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import towhee\n",
    "\n",
    "(\n",
    "    towhee.glob['path']('./train/tap_dancing/*.mp4')\n",
    "          .video_decode.ffmpeg['path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 16})\n",
    "          .action_classification['frames', ('predicts', 'scores', 'features')].pytorchvideo(\n",
    "              model_name='x3d_m', skip_preprocess=True, topk=5)\n",
    "          .select['path', 'predicts', 'scores']()\n",
    "          .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b987a6e",
   "metadata": {},
   "source": [
    "#### Pipeline Explanation\n",
    "\n",
    "Here are some details for each line of the assemble pipeline:\n",
    "\n",
    "- `towhee.read_csv()`: read tabular data from csv file\n",
    "\n",
    "\n",
    "- `.video_decode.ffmpeg()`: an embeded Towhee operator reading video as frames with specified sample method and number of samples. [learn more](https://towhee.io/video-decode/ffmpeg)\n",
    "\n",
    "- `.action_classification.pytorchvideo()`: an embeded Towhee operator applying specified model to video frames, which can be used to predict labels and extract features for video. [learn more](https://towhee.io/action-classification/pytorchvideo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b04bc12",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We have just showed how to classify video, but how's its performance? Towhee has provided different options for metrics to evaluate predicted results against ground truths.\n",
    "\n",
    "In this section, we'll measure the performance with the average metric value:\n",
    "\n",
    "- **mHR (recall@K):**\n",
    "    - Mean Hit Ratio describes how many actual relevant results are returned out of all ground truths.\n",
    "    - Since we predict top K labels while only 1 ground truth for each entity, the mean hit ratio is equivalent to recall@topk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2525c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mengjia.gu/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 92.92158579826355\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_hit_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>top1</th>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top3</th>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top5</th>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_hit_ratio\n",
       "top1           0.700\n",
       "top3           0.875\n",
       "top5           0.900"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "dc = (\n",
    "    towhee.read_csv('reverse_video_search.csv').unstream()\n",
    "          .video_decode.ffmpeg['path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 16})\n",
    "          .action_classification['frames', ('predicts', 'scores', 'features')].pytorchvideo(\n",
    "              model_name='x3d_m', skip_preprocess=True, topk=5)\n",
    ")\n",
    "end = time.time()\n",
    "print(f'Total time: {end-start}')\n",
    "\n",
    "benchmark = (\n",
    "    dc.runas_op['path', 'ground_truth'](func=ground_truth)\n",
    "      .runas_op['predicts', 'top1'](func=lambda x: x[:1])\n",
    "      .runas_op['predicts', 'top3'](func=lambda x: x[:3])\n",
    "      .with_metrics(['mean_hit_ratio'])\n",
    "      .evaluate['ground_truth', 'top1'](name='top1')\n",
    "      .evaluate['ground_truth', 'top3'](name='top3')\n",
    "      .evaluate['ground_truth', 'predicts'](name='top5')\n",
    "      .report()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80f350",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "You're always encouraged to play around with the tutorial. We present some optimization options here to make improvements in accuracy, latency, and resource usage. With these methods, you can make the classification system better in performance and more feasible in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924813d7",
   "metadata": {},
   "source": [
    "### Change model\n",
    "\n",
    "There are more video models using different networks. Normally a more complicated or larger model will show better results while cost more. You can always try more models to tradeoff among accuracy, latency, and resource usage. Here I show the performance of video classification using a SOTA model with [multiscale vision transformer](https://arxiv.org/abs/2104.11227) as backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b10ba34e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mengjia.gu/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_hit_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>top1</th>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top3</th>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top5</th>\n",
       "      <td>0.920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_hit_ratio\n",
       "top1           0.745\n",
       "top3           0.900\n",
       "top5           0.920"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "benchmark = (\n",
    "    towhee.read_csv('reverse_video_search.csv').unstream()\n",
    "          .video_decode.ffmpeg['path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 32})\n",
    "          .action_classification['frames', ('predicts', 'scores', 'features')].pytorchvideo(\n",
    "              model_name='mvit_base_32x3', skip_preprocess=True, topk=5)\n",
    "          .runas_op['path', 'ground_truth'](func=ground_truth)\n",
    "          .runas_op['predicts', 'top1'](func=lambda x: x[:1])\n",
    "          .runas_op['predicts', 'top3'](func=lambda x: x[:3])\n",
    "          .with_metrics(['mean_hit_ratio'])\n",
    "          .evaluate['ground_truth', 'top1'](name='top1')\n",
    "          .evaluate['ground_truth', 'top3'](name='top3')\n",
    "          .evaluate['ground_truth', 'predicts'](name='top5')\n",
    "          .report()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d18911",
   "metadata": {},
   "source": [
    "### Parallel Execution\n",
    "\n",
    "We are able to enable parallel execution by simply calling set_parallel within the pipeline. It tells Towhee to process the data in parallel. The code below enables parallel execution on the above example. It shows that it finishes the classification of 200 videos within 2 seconds with 5 parallel executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "235656b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mengjia.gu/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1.6542744636535645\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dc = (\n",
    "    towhee.read_csv('reverse_video_search.csv')\n",
    "          .set_parallel(5)\n",
    "          .video_decode.ffmpeg['path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 16})\n",
    "          .action_classification['frames', ('predicts', 'scores', 'features')].pytorchvideo(\n",
    "              model_name='x3d_m', skip_preprocess=True, topk=5)\n",
    ")\n",
    "end = time.time()\n",
    "print(f'Total time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac47c2",
   "metadata": {},
   "source": [
    "### Exception Safe\n",
    "\n",
    "When we have large-scale data, there may be some bad data that will cause errors. Typically, the users don't want such errors to break the system in production. Therefore, the pipeline should continue to process the rest of the videos and report broken ones.\n",
    "\n",
    "Towhee supports an `exception-safe` execution mode that allows the pipeline to continue on exceptions and represent the exceptions with Empty values. The user can choose how to deal with the empty values at the end of the pipeline. During the query below, there are 4 files in total under the exception folder, one of them is broken. With `exception-safe`, it will print the ERROR but NOT terminate the process. As you can see from results, `drop_empty` deletes empty data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d530b7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mengjia.gu/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "2022-06-08 21:51:35,523 - 139916792783424 - video_decoder.py-video_decoder:121 - ERROR: moov atom not found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border-collapse: collapse;\"><tr><th style=\"text-align: center; font-size: 130%; border: none;\">path</th> <th style=\"text-align: center; font-size: 130%; border: none;\">labels</th></tr> <tr><td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">./exception/kDuAS29BCwk.mp4</td> <td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[chopping wood,sword fighting,throwing axe,walking the dog,...] len=5</td></tr> <tr><td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">./exception/ty4UQlowp0c.mp4</td> <td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[eating carrots,eating spaghetti,shaking head,eating chips,...] len=5</td></tr> <tr><td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">./exception/rJu8mSNHX_8.mp4</td> <td style=\"text-align: center; vertical-align: center; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[shaking head,finger snapping,laughing,eating ice cream,...] len=5</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(\n",
    "    towhee.glob['path']('./exception/*')\n",
    "          .exception_safe()\n",
    "          .video_decode.ffmpeg['path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 16})\n",
    "          .action_classification['frames', ('labels', 'scores', 'vec')].pytorchvideo(\n",
    "              model_name='x3d_m', skip_preprocess=True)\n",
    "          .drop_empty()\n",
    "          .select['path', 'labels']()\n",
    "          .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607783a1",
   "metadata": {},
   "source": [
    "## Release a Showcase\n",
    "\n",
    "We've learnt how to build a reverse video search engine. Now it's time to add some interface and release a showcase. Towhee provides `towhee.api()` to wrap the data processing pipeline as a function with `.as_function()`. So we can build a quick demo with this `action_classification_function` with [Gradio](https://gradio.app/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a78c9ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mengjia.gu/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866/\n",
      "Running on public URL: https://56923.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"https://56923.gradio.app\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f3f3ee51cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7f3f3d9944c0>,\n",
       " 'http://127.0.0.1:7866/',\n",
       " 'https://56923.gradio.app')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio\n",
    "\n",
    "topk = 3\n",
    "with towhee.api() as api:\n",
    "    action_classification_function = (\n",
    "         api.video_decode.ffmpeg(\n",
    "                sample_type='uniform_temporal_subsample', args={'num_samples': 32})\n",
    "            .action_classification.pytorchvideo(model_name='mvit_base_32x3', skip_preprocess=True, topk=topk)\n",
    "            .runas_op(func=lambda res: {res[0][i]: res[1][i] for i in range(len(res[0]))})\n",
    "            .as_function()\n",
    "    )\n",
    "    \n",
    "\n",
    "interface = gradio.Interface(action_classification_function, \n",
    "                             inputs=gradio.Video(source='upload'),\n",
    "                             outputs=[gradio.Label(num_top_classes=topk)]\n",
    "                            )\n",
    "\n",
    "\n",
    "interface.launch(inline=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef0b4e",
   "metadata": {},
   "source": [
    "<img src='action_classification_demo.png' alt='action_classification_demo' width=700px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74dfea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mg-dev",
   "language": "python",
   "name": "mg-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
